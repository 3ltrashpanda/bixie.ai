{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Bixie Model Training with Progress Monitoring\n",
    "\n",
    "This notebook includes:\n",
    "- Progress bars and monitoring\n",
    "- Checkpointing to save progress\n",
    "- Ability to resume from where it left off\n",
    "- Multiple embedding models\n",
    "- Binary file analysis\n",
    "- Training data sources for binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, roc_auc_score, precision_recall_curve, precision_score, recall_score\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, \"/home/trashpanda/repos/bixie.ai/\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Embedding Models with Progress Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, \n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    BertTokenizer, BertModel,\n",
    "    DistilBertTokenizer, DistilBertModel,\n",
    "    T5Tokenizer, T5EncoderModel\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModelEmbedder:\n",
    "    \"\"\"Enhanced embedder with progress monitoring and checkpointing\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"microsoft/codebert-base\", device=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Loading model: {model_name} on {self.device}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = 512\n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "        print(f\"Model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def embed_text(self, text, pooling_strategy='mean'):\n",
    "        \"\"\"Embed text using different pooling strategies\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                \n",
    "                if pooling_strategy == 'mean':\n",
    "                    embedding = hidden_states.mean(dim=1)\n",
    "                elif pooling_strategy == 'cls':\n",
    "                    embedding = hidden_states[:, 0, :]\n",
    "                elif pooling_strategy == 'max':\n",
    "                    embedding = hidden_states.max(dim=1)[0]\n",
    "                elif pooling_strategy == 'attention':\n",
    "                    attention_weights = F.softmax(hidden_states.mean(dim=-1), dim=-1)\n",
    "                    embedding = (hidden_states * attention_weights.unsqueeze(-1)).sum(dim=1)\n",
    "                \n",
    "                return embedding.squeeze().cpu().numpy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Embedding failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_binary_string(self, code_bytes):\n",
    "        \"\"\"Embed binary data by converting to text representation\"\"\"\n",
    "        try:\n",
    "            hex_str = code_bytes.hex()\n",
    "            formatted_hex = ' '.join([hex_str[i:i+2] for i in range(0, len(hex_str), 2)])\n",
    "            return self.embed_text(formatted_hex)\n",
    "        except Exception as e:\n",
    "            print(f\"Binary embedding failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Model configurations to test\n",
    "MODEL_CONFIGS = {\n",
    "    'codebert': 'microsoft/codebert-base',\n",
    "    'roberta': 'roberta-base',\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'distilbert': 'distilbert-base-uncased'\n",
    "}\n",
    "\n",
    "# Pooling strategies\n",
    "POOLING_STRATEGIES = ['mean', 'cls', 'max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Progress Monitoring and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCheckpointer:\n",
    "    \"\"\"Manages checkpointing for embedding extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir=\"../checkpoints\"):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, embeddings, labels, model_name, pooling, batch_idx, total_batches):\n",
    "        \"\"\"Save current progress\"\"\"\n",
    "        checkpoint = {\n",
    "            'embeddings': embeddings,\n",
    "            'labels': labels,\n",
    "            'model_name': model_name,\n",
    "            'pooling': pooling,\n",
    "            'batch_idx': batch_idx,\n",
    "            'total_batches': total_batches,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        checkpoint_file = self.checkpoint_dir / f\"{model_name}_{pooling}_checkpoint.pkl\"\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "        \n",
    "        print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "        \n",
    "    def load_checkpoint(self, model_name, pooling):\n",
    "        \"\"\"Load existing checkpoint\"\"\"\n",
    "        checkpoint_file = self.checkpoint_dir / f\"{model_name}_{pooling}_checkpoint.pkl\"\n",
    "        \n",
    "        if checkpoint_file.exists():\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint = pickle.load(f)\n",
    "            print(f\"Loaded checkpoint: {checkpoint_file}\")\n",
    "            print(f\"Progress: {checkpoint['batch_idx']}/{checkpoint['total_batches']} batches\")\n",
    "            return checkpoint\n",
    "        return None\n",
    "    \n",
    "    def clear_checkpoint(self, model_name, pooling):\n",
    "        \"\"\"Clear checkpoint after successful completion\"\"\"\n",
    "        checkpoint_file = self.checkpoint_dir / f\"{model_name}_{pooling}_checkpoint.pkl\"\n",
    "        if checkpoint_file.exists():\n",
    "            checkpoint_file.unlink()\n",
    "            print(f\"Cleared checkpoint: {checkpoint_file}\")\n",
    "\n",
    "def extract_embeddings_with_progress(texts, labels, embedder, pooling_strategy='mean', \n",
    "                                     batch_size=50, checkpoint_every=10, resume=True):\n",
    "    \"\"\"Extract embeddings with progress monitoring and checkpointing\"\"\"\n",
    "    \n",
    "    checkpointer = EmbeddingCheckpointer()\n",
    "    model_name = embedder.model_name.split('/')[-1]\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    start_batch = 0\n",
    "    X, y = [], []\n",
    "    \n",
    "    if resume:\n",
    "        checkpoint = checkpointer.load_checkpoint(model_name, pooling_strategy)\n",
    "        if checkpoint:\n",
    "            X = checkpoint['embeddings']\n",
    "            y = checkpoint['labels']\n",
    "            start_batch = checkpoint['batch_idx']\n",
    "            print(f\"Resuming from batch {start_batch}\")\n",
    "    \n",
    "    # Calculate batches\n",
    "    total_samples = len(texts)\n",
    "    total_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Total batches: {total_batches}\")\n",
    "    print(f\"Starting from batch: {start_batch}\")\n",
    "    \n",
    "    failed_count = 0\n",
    "    \n",
    "    # Process batches with progress bar\n",
    "    for batch_idx in tqdm(range(start_batch, total_batches), desc=f\"Embedding ({model_name}, {pooling_strategy})\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        \n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        batch_labels_processed = []\n",
    "        \n",
    "        for i, (text, label) in enumerate(zip(batch_texts, batch_labels)):\n",
    "            try:\n",
    "                text_bytes = text.encode('utf-8')\n",
    "                emb = embedder.embed_binary_string(text_bytes)\n",
    "                \n",
    "                if emb is not None:\n",
    "                    batch_embeddings.append(emb)\n",
    "                    batch_labels_processed.append(label)\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"Failed to embed sample {start_idx + i}: {e}\")\n",
    "        \n",
    "        # Add batch results to main lists\n",
    "        X.extend(batch_embeddings)\n",
    "        y.extend(batch_labels_processed)\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (batch_idx + 1) % checkpoint_every == 0:\n",
    "            checkpointer.save_checkpoint(X, y, model_name, pooling_strategy, batch_idx + 1, total_batches)\n",
    "            \n",
    "        # Print progress every 5 batches\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{total_batches}: {len(X)} embeddings, {failed_count} failed\")\n",
    "    \n",
    "    # Clear checkpoint after successful completion\n",
    "    checkpointer.clear_checkpoint(model_name, pooling_strategy)\n",
    "    \n",
    "    print(f\"\\nEmbedding completed!\")\n",
    "    print(f\"Successful embeddings: {len(X)}\")\n",
    "    print(f\"Failed embeddings: {failed_count}\")\n",
    "    print(f\"Success rate: {len(X)/(len(X)+failed_count)*100:.2f}%\")\n",
    "    \n",
    "    return np.array(X), np.array(y), failed_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 22734\n",
      "Vulnerable samples: 2240\n",
      "Clean samples: 20494\n",
      "Class balance: 0.099\n",
      "\n",
      "Training samples: 18187\n",
      "Test samples: 4547\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "with open(\"../datasets/training_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [item[\"code\"] for item in data]\n",
    "labels = [item[\"label\"] for item in data]\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Vulnerable samples: {sum(labels)}\")\n",
    "print(f\"Clean samples: {len(labels) - sum(labels)}\")\n",
    "print(f\"Class balance: {sum(labels)/len(labels):.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Model Evaluation with Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing codebert: microsoft/codebert-base\n",
      "============================================================\n",
      "Loading model: microsoft/codebert-base on cpu\n",
      "Model loaded successfully. Embedding dimension: 768\n",
      "\n",
      "--- Testing mean pooling ---\n",
      "Total samples: 18187\n",
      "Batch size: 50\n",
      "Total batches: 364\n",
      "Starting from batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding (codebert-base, mean):   0%|          | 0/364 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Store results for comparison\n",
    "all_results = []\n",
    "\n",
    "# Test different models and pooling strategies\n",
    "for model_name, model_path in MODEL_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {model_name}: {model_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        embedder = MultiModelEmbedder(model_path)\n",
    "        \n",
    "        for pooling in POOLING_STRATEGIES:\n",
    "            print(f\"\\n--- Testing {pooling} pooling ---\")\n",
    "            \n",
    "            # Extract embeddings with progress monitoring\n",
    "            X_train_emb, y_train_emb, failed_train = extract_embeddings_with_progress(\n",
    "                X_train, y_train, embedder, pooling, batch_size=50, checkpoint_every=10\n",
    "            )\n",
    "            \n",
    "            X_test_emb, y_test_emb, failed_test = extract_embeddings_with_progress(\n",
    "                X_test, y_test, embedder, pooling, batch_size=50, checkpoint_every=10\n",
    "            )\n",
    "            \n",
    "            if len(X_train_emb) < 10:\n",
    "                print(f\"Skipping {model_name} with {pooling} - insufficient samples\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nTraining embeddings: {len(X_train_emb)}\")\n",
    "            print(f\"Test embeddings: {len(X_test_emb)}\")\n",
    "            \n",
    "            # Test multiple classifiers\n",
    "            classifiers = {\n",
    "                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "                'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "            }\n",
    "            \n",
    "            for clf_name, clf in classifiers.items():\n",
    "                try:\n",
    "                    print(f\"\\nTraining {clf_name}...\")\n",
    "                    clf.fit(X_train_emb, y_train_emb)\n",
    "                    y_pred = clf.predict(X_test_emb)\n",
    "                    y_proba = clf.predict_proba(X_test_emb)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "                    \n",
    "                    results = evaluate_model(\n",
    "                        y_test_emb, y_pred, y_proba, \n",
    "                        f\"{model_name}_{pooling}_{clf_name}\"\n",
    "                    )\n",
    "                    results['embedding_model'] = model_name\n",
    "                    results['pooling'] = pooling\n",
    "                    results['classifier'] = clf_name\n",
    "                    all_results.append(results)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to train {clf_name}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # ROC AUC if probabilities available\n",
    "    auc = None\n",
    "    if y_proba is not None:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    if auc:\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display best performing models\n",
    "print(\"\\nTop 10 Models by F1 Score:\")\n",
    "top_models = results_df.nlargest(10, 'f1_score')\n",
    "print(top_models[['model', 'embedding_model', 'pooling', 'classifier', 'f1_score', 'accuracy', 'auc']])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# F1 Score comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=results_df, x='embedding_model', y='f1_score')\n",
    "plt.title('F1 Score by Embedding Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=results_df, x='classifier', y='accuracy')\n",
    "plt.title('Accuracy by Classifier')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Pooling strategy comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=results_df, x='pooling', y='f1_score')\n",
    "plt.title('F1 Score by Pooling Strategy')\n",
    "\n",
    "# ROC AUC comparison (if available)\n",
    "plt.subplot(2, 2, 4)\n",
    "auc_data = results_df.dropna(subset=['auc'])\n",
    "if len(auc_data) > 0:\n",
    "    sns.boxplot(data=auc_data, x='embedding_model', y='auc')\n",
    "    plt.title('ROC AUC by Embedding Model')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Binary File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "class BinaryAnalyzer:\n",
    "    \"\"\"Specialized analyzer for binary files\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def extract_strings(self, binary_path):\n",
    "        \"\"\"Extract strings from binary\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['strings', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=30)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract strings: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_hex_dump(self, binary_path, max_bytes=4096):\n",
    "        \"\"\"Extract hex dump from binary\"\"\"\n",
    "        try:\n",
    "            with open(binary_path, 'rb') as f:\n",
    "                data = f.read(max_bytes)\n",
    "            return data.hex()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract hex dump: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_disassembly(self, binary_path):\n",
    "        \"\"\"Extract disassembly using objdump\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['objdump', '-d', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=60)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract disassembly: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_binary(self, binary_path):\n",
    "        \"\"\"Comprehensive binary analysis\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        # Extract different representations\n",
    "        analysis['strings'] = self.extract_strings(binary_path)\n",
    "        analysis['hex_dump'] = self.extract_hex_dump(binary_path)\n",
    "        analysis['disassembly'] = self.extract_disassembly(binary_path)\n",
    "        \n",
    "        # Create combined representation\n",
    "        combined_text = f\"STRINGS:\\n{analysis['strings']}\\n\\nHEX:\\n{analysis['hex_dump']}\\n\\nASM:\\n{analysis['disassembly']}\"\n",
    "        \n",
    "        # Embed the combined representation\n",
    "        embedding = self.embedder.embed_text(combined_text)\n",
    "        \n",
    "        return embedding, analysis\n",
    "\n",
    "# Example usage\n",
    "print(\"Binary analyzer initialized with multiple extraction methods\")\n",
    "print(\"Methods available:\")\n",
    "print(\"- String extraction\")\n",
    "print(\"- Hex dump extraction\")\n",
    "print(\"- Disassembly extraction\")\n",
    "print(\"- Combined analysis with embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Data Sources for Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "## Binary Vulnerability Training Data Sources:\n",
    "\n",
    "### 1. Public Vulnerability Databases:\n",
    "- CVE Database (https://cve.mitre.org/)\n",
    "- NVD (https://nvd.nist.gov/)\n",
    "- Exploit-DB (https://www.exploit-db.com/)\n",
    "- SecurityFocus (https://www.securityfocus.com/)\n",
    "\n",
    "### 2. Malware Datasets:\n",
    "- VirusTotal (https://www.virustotal.com/) - API access\n",
    "- MalwareBazaar (https://bazaar.abuse.ch/)\n",
    "- VX-Underground (https://vx-underground.org/)\n",
    "- MalShare (https://malshare.com/)\n",
    "\n",
    "### 3. CTF and Security Challenges:\n",
    "- Pwnable.kr\n",
    "- Pwnable.tw\n",
    "- HackTheBox binaries\n",
    "- VulnHub challenges\n",
    "\n",
    "### 4. Academic Datasets:\n",
    "- Microsoft Malware Classification Challenge\n",
    "- Drebin Dataset (Android malware)\n",
    "- Malimg Dataset\n",
    "- EMBER Dataset (https://github.com/endgameinc/ember)\n",
    "\n",
    "### 5. Open Source Projects:\n",
    "- Known vulnerable versions of popular software\n",
    "- Historical releases with known CVEs\n",
    "- Fuzzing results from OSS-Fuzz\n",
    "\n",
    "### 6. Data Collection Strategies:\n",
    "1. Version comparison: Compare vulnerable vs patched versions\n",
    "2. Fuzzing: Generate crash samples\n",
    "3. Symbolic execution: Extract vulnerable paths\n",
    "4. Static analysis: Identify vulnerable patterns\n",
    "5. Dynamic analysis: Runtime vulnerability detection\n",
    "\n",
    "### 7. Labeling Strategies:\n",
    "- CVE mapping to binary versions\n",
    "- Exploit availability as vulnerability indicator\n",
    "- Patch analysis for vulnerability confirmation\n",
    "- Static analysis tool results\n",
    "- Dynamic analysis results\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enhanced Model Inference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "# Enhanced model_inference.py Implementation\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import subprocess\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from bixie.vector_store.chroma_store import ChromaStore\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MODEL_NAME = \"microsoft/codebert-base\"\n",
    "CLASSIFIER_PATH = Path(\"bixie.ai/fine_tuned_model.pkl\")\n",
    "NEURAL_MODEL_PATH = Path(\"bixie.ai/neural_classifier.pth\")\n",
    "\n",
    "logger = logging.getLogger(\"bixie.model_inference\")\n",
    "\n",
    "class MultiModelEmbedder:\n",
    "    \"\"\"Enhanced embedder supporting multiple models and binary analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = DEFAULT_MODEL_NAME, device=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Loading model: {model_name} on {self.device}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = 512\n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "        logger.info(f\"Model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    def embed_text(self, text: str, pooling_strategy: str = 'mean') -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed text using different pooling strategies\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                \n",
    "                if pooling_strategy == 'mean':\n",
    "                    embedding = hidden_states.mean(dim=1)\n",
    "                elif pooling_strategy == 'cls':\n",
    "                    embedding = hidden_states[:, 0, :]\n",
    "                elif pooling_strategy == 'max':\n",
    "                    embedding = hidden_states.max(dim=1)[0]\n",
    "                elif pooling_strategy == 'attention':\n",
    "                    attention_weights = torch.softmax(hidden_states.mean(dim=-1), dim=-1)\n",
    "                    embedding = (hidden_states * attention_weights.unsqueeze(-1)).sum(dim=1)\n",
    "                \n",
    "                return embedding.squeeze().cpu().numpy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Text embedding failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_binary_string(self, code_bytes: bytes) -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed binary data\"\"\"\n",
    "        try:\n",
    "            hex_str = code_bytes.hex()\n",
    "            formatted_hex = ' '.join([hex_str[i:i+2] for i in range(0, len(hex_str), 2)])\n",
    "            return self.embed_text(formatted_hex)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Binary embedding failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_file(self, filepath: Path) -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed file with automatic format detection\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Try to decode as text first\n",
    "            try:\n",
    "                text_content = content.decode('utf-8')\n",
    "                return self.embed_text(text_content)\n",
    "            except UnicodeDecodeError:\n",
    "                # Binary file - use binary embedding\n",
    "                return self.embed_binary_string(content)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to embed {filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "class BinaryAnalyzer:\n",
    "    \"\"\"Specialized binary file analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: MultiModelEmbedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def extract_strings(self, binary_path: Path) -> str:\n",
    "        \"\"\"Extract strings from binary\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['strings', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=30)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract strings: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_hex_dump(self, binary_path: Path, max_bytes: int = 4096) -> str:\n",
    "        \"\"\"Extract hex dump\"\"\"\n",
    "        try:\n",
    "            with open(binary_path, 'rb') as f:\n",
    "                data = f.read(max_bytes)\n",
    "            return data.hex()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract hex dump: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_disassembly(self, binary_path: Path) -> str:\n",
    "        \"\"\"Extract disassembly\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['objdump', '-d', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=60)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract disassembly: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_binary(self, binary_path: Path) -> Optional[np.ndarray]:\n",
    "        \"\"\"Comprehensive binary analysis\"\"\"\n",
    "        try:\n",
    "            strings = self.extract_strings(binary_path)\n",
    "            hex_dump = self.extract_hex_dump(binary_path)\n",
    "            disassembly = self.extract_disassembly(binary_path)\n",
    "            \n",
    "            # Combine all representations\n",
    "            combined_text = f\"STRINGS:\\n{strings}\\n\\nHEX:\\n{hex_dump}\\n\\nASM:\\n{disassembly}\"\n",
    "            \n",
    "            return self.embedder.embed_text(combined_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Binary analysis failed for {binary_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "class VulnerabilityClassifier(nn.Module):\n",
    "    \"\"\"Neural network classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self.network(x))\n",
    "\n",
    "def load_classifier(classifier_path: Path = CLASSIFIER_PATH) -> Optional[Union[object, VulnerabilityClassifier]]:\n",
    "    \"\"\"Load trained classifier (traditional ML or neural network)\"\"\"\n",
    "    if classifier_path.exists():\n",
    "        try:\n",
    "            clf = joblib.load(classifier_path)\n",
    "            logger.info(f\"Loaded traditional classifier from {classifier_path}\")\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load traditional classifier: {e}\")\n",
    "    \n",
    "    # Try loading neural network\n",
    "    neural_path = NEURAL_MODEL_PATH\n",
    "    if neural_path.exists():\n",
    "        try:\n",
    "            # Load model architecture and weights\n",
    "            # This would need to be implemented based on saved model structure\n",
    "            logger.info(f\"Loaded neural classifier from {neural_path}\")\n",
    "            return None  # Placeholder\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load neural classifier: {e}\")\n",
    "    \n",
    "    logger.warning(f\"No trained classifier found\")\n",
    "    return None\n",
    "\n",
    "def run_model_inference(\n",
    "    target_paths: List[Path],\n",
    "    output_dir: Optional[Path] = None,\n",
    "    save_vectors: bool = True,\n",
    "    chroma_store: Optional[ChromaStore] = None,\n",
    "    model_name: str = DEFAULT_MODEL_NAME,\n",
    "    pooling_strategy: str = 'mean',\n",
    "    batch_size: int = 50,\n",
    "    show_progress: bool = True\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run ML model inference with progress monitoring and checkpointing\n",
    "    \"\"\"\n",
    "    embedder = MultiModelEmbedder(model_name)\n",
    "    classifier = load_classifier()\n",
    "    binary_analyzer = BinaryAnalyzer(embedder)\n",
    "    results = []\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    iterator = tqdm(target_paths, desc=\"Processing files\") if show_progress else target_paths\n",
    "    \n",
    "    for path in iterator:\n",
    "        if not path.is_file():\n",
    "            logger.warning(f\"Skipping non-file target: {path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Check if file is already processed\n",
    "            file_hash = hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "            \n",
    "            if chroma_store and chroma_store.exists(file_hash):\n",
    "                logger.info(f\"Skipping already processed file: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze binary\n",
    "            vector = binary_analyzer.analyze_binary(path)\n",
    "            \n",
    "            if vector is None:\n",
    "                logger.warning(f\"Failed to analyze {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Save to Chroma store\n",
    "            if chroma_store:\n",
    "                chroma_store.add(file_hash, vector)\n",
    "            \n",
    "            # Run classifier\n",
    "            if classifier:\n",
    "                prediction = classifier.predict([vector])[0]\n",
    "                result = {\n",
    "                    'path': str(path),\n",
    "                    'hash': file_hash,\n",
    "                    'prediction': prediction,\n",
    "                    'vector': vector.tolist()\n",
    "                }\n",
    "            \n",
    "            # Save to output directory\n",
    "            if output_dir:\n",
    "                output_path = output_dir / path.name\n",
    "                output_path.write_bytes(path.read_bytes())\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Enhanced Model Inference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "# Enhanced model_inference.py Implementation\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import subprocess\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from bixie.vector_store.chroma_store import ChromaStore\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MODEL_NAME = \"microsoft/codebert-base\"\n",
    "CLASSIFIER_PATH = Path(\"bixie.ai/fine_tuned_model.pkl\")\n",
    "NEURAL_MODEL_PATH = Path(\"bixie.ai/neural_classifier.pth\")\n",
    "\n",
    "logger = logging.getLogger(\"bixie.model_inference\")\n",
    "\n",
    "class MultiModelEmbedder:\n",
    "    \"\"\"Enhanced embedder supporting multiple models and binary analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = DEFAULT_MODEL_NAME, device=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Loading model: {model_name} on {self.device}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = 512\n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "        logger.info(f\"Model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    def embed_text(self, text: str, pooling_strategy: str = 'mean') -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed text using different pooling strategies\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                \n",
    "                if pooling_strategy == 'mean':\n",
    "                    embedding = hidden_states.mean(dim=1)\n",
    "                elif pooling_strategy == 'cls':\n",
    "                    embedding = hidden_states[:, 0, :]\n",
    "                elif pooling_strategy == 'max':\n",
    "                    embedding = hidden_states.max(dim=1)[0]\n",
    "                elif pooling_strategy == 'attention':\n",
    "                    attention_weights = torch.softmax(hidden_states.mean(dim=-1), dim=-1)\n",
    "                    embedding = (hidden_states * attention_weights.unsqueeze(-1)).sum(dim=1)\n",
    "                \n",
    "                return embedding.squeeze().cpu().numpy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Text embedding failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_binary_string(self, code_bytes: bytes) -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed binary data\"\"\"\n",
    "        try:\n",
    "            hex_str = code_bytes.hex()\n",
    "            formatted_hex = ' '.join([hex_str[i:i+2] for i in range(0, len(hex_str), 2)])\n",
    "            return self.embed_text(formatted_hex)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Binary embedding failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_file(self, filepath: Path) -> Optional[np.ndarray]:\n",
    "        \"\"\"Embed file with automatic format detection\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Try to decode as text first\n",
    "            try:\n",
    "                text_content = content.decode('utf-8')\n",
    "                return self.embed_text(text_content)\n",
    "            except UnicodeDecodeError:\n",
    "                # Binary file - use binary embedding\n",
    "                return self.embed_binary_string(content)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to embed {filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "class BinaryAnalyzer:\n",
    "    \"\"\"Specialized binary file analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: MultiModelEmbedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def extract_strings(self, binary_path: Path) -> str:\n",
    "        \"\"\"Extract strings from binary\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['strings', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=30)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract strings: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_hex_dump(self, binary_path: Path, max_bytes: int = 4096) -> str:\n",
    "        \"\"\"Extract hex dump\"\"\"\n",
    "        try:\n",
    "            with open(binary_path, 'rb') as f:\n",
    "                data = f.read(max_bytes)\n",
    "            return data.hex()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract hex dump: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_disassembly(self, binary_path: Path) -> str:\n",
    "        \"\"\"Extract disassembly\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['objdump', '-d', str(binary_path)], \n",
    "                                   capture_output=True, text=True, timeout=60)\n",
    "            return result.stdout\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract disassembly: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_binary(self, binary_path: Path) -> Optional[np.ndarray]:\n",
    "        \"\"\"Comprehensive binary analysis\"\"\"\n",
    "        try:\n",
    "            strings = self.extract_strings(binary_path)\n",
    "            hex_dump = self.extract_hex_dump(binary_path)\n",
    "            disassembly = self.extract_disassembly(binary_path)\n",
    "            \n",
    "            # Combine all representations\n",
    "            combined_text = f\"STRINGS:\\n{strings}\\n\\nHEX:\\n{hex_dump}\\n\\nASM:\\n{disassembly}\"\n",
    "            \n",
    "            return self.embedder.embed_text(combined_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Binary analysis failed for {binary_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "class VulnerabilityClassifier(nn.Module):\n",
    "    \"\"\"Neural network classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self.network(x))\n",
    "\n",
    "def load_classifier(classifier_path: Path = CLASSIFIER_PATH) -> Optional[Union[object, VulnerabilityClassifier]]:\n",
    "    \"\"\"Load trained classifier (traditional ML or neural network)\"\"\"\n",
    "    if classifier_path.exists():\n",
    "        try:\n",
    "            clf = joblib.load(classifier_path)\n",
    "            logger.info(f\"Loaded traditional classifier from {classifier_path}\")\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load traditional classifier: {e}\")\n",
    "    \n",
    "    # Try loading neural network\n",
    "    neural_path = NEURAL_MODEL_PATH\n",
    "    if neural_path.exists():\n",
    "        try:\n",
    "            # Load model architecture and weights\n",
    "            # This would need to be implemented based on saved model structure\n",
    "            logger.info(f\"Loaded neural classifier from {neural_path}\")\n",
    "            return None  # Placeholder\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load neural classifier: {e}\")\n",
    "    \n",
    "    logger.warning(f\"No trained classifier found\")\n",
    "    return None\n",
    "\n",
    "def run_model_inference(\n",
    "    target_paths: List[Path],\n",
    "    output_dir: Optional[Path] = None,\n",
    "    save_vectors: bool = True,\n",
    "    chroma_store: Optional[ChromaStore] = None,\n",
    "    model_name: str = DEFAULT_MODEL_NAME,\n",
    "    pooling_strategy: str = 'mean',\n",
    "    batch_size: int = 50,\n",
    "    show_progress: bool = True\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run ML model inference with progress monitoring and checkpointing\n",
    "    \"\"\"\n",
    "    embedder = MultiModelEmbedder(model_name)\n",
    "    classifier = load_classifier()\n",
    "    binary_analyzer = BinaryAnalyzer(embedder)\n",
    "    results = []\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    iterator = tqdm(target_paths, desc=\"Processing files\") if show_progress else target_paths\n",
    "    \n",
    "    for path in iterator:\n",
    "        if not path.is_file():\n",
    "            logger.warning(f\"Skipping non-file target: {path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Check if file is already processed\n",
    "            file_hash = hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "            \n",
    "            if chroma_store and chroma_store.exists(file_hash):\n",
    "                logger.info(f\"Skipping already processed file: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze binary\n",
    "            vector = binary_analyzer.analyze_binary(path)\n",
    "            \n",
    "            if vector is None:\n",
    "                logger.warning(f\"Failed to analyze {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Save to Chroma store\n",
    "            if chroma_store:\n",
    "                chroma_store.add(file_hash, vector)\n",
    "            \n",
    "            # Run classifier\n",
    "            if classifier:\n",
    "                prediction = classifier.predict([vector])[0]\n",
    "                result = {\n",
    "                    'path': str(path),\n",
    "                    'hash': file_hash,\n",
    "                    'prediction': prediction,\n",
    "                    'vector': vector.tolist()\n",
    "                }\n",
    "            \n",
    "            # Save to output directory\n",
    "            if output_dir:\n",
    "                output_path = output_dir / path.name\n",
    "                output_path.write_bytes(path.read_bytes())\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bixie.ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
